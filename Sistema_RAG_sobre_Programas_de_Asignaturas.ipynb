{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6RPompjyNQ2l",
      "metadata": {
        "id": "6RPompjyNQ2l"
      },
      "source": [
        "# üìö Propuesta de Trabajo: Sistema RAG con Base Vectorial sobre programas de asignaturas de la UNLu\n",
        "Se propone la implementaci√≥n de un sistema de Recuperaci√≥n Aumentada por Generaci√≥n (RAG) utilizando como colecci√≥n de documentos los Programas de distintas asignaturas de carreras de la Universidad Nacional de Luj√°n.\n",
        "\n",
        "El sistema se construir√° mediante el uso de embeddings, el motor de b√∫squeda vectorial Chroma, y un modelo de lenguaje de Llama para generar respuestas precisas basadas en la informaci√≥n recuperada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CYrXx-zGLo1Q",
      "metadata": {
        "id": "CYrXx-zGLo1Q"
      },
      "source": [
        "## üìò Introducci√≥n: LlamaIndex y RAG\n",
        "\n",
        "### üß† ¬øQu√© es LlamaIndex?\n",
        "**LlamaIndex** es un framework dise√±ado para construir aplicaciones basadas en modelos de lenguaje (LLM) que utilizan **informaci√≥n externa y espec√≠fica de dominio**.  \n",
        "Permite extender las capacidades de los LLMs integrando datos privados, actualizados o especializados.\n",
        "\n",
        "### üîé ¬øQu√© es Retrieval-Augmented Generation (RAG)?\n",
        "Los modelos de lenguaje tradicionales tienen un conocimiento limitado a los datos p√∫blicos con los que fueron entrenados.\n",
        "\n",
        "La **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** soluciona este problema incorporando informaci√≥n relevante y din√°mica desde fuentes externas (documentos, bases de datos, APIs, etc.), justo en el momento de la consulta.\n",
        "\n",
        "Este enfoque representa un cambio importante:\n",
        "- Las respuestas **no dependen solo del conocimiento entrenado** en el modelo.\n",
        "- Se incorpora **contexto recuperado en tiempo real**, lo que mejora precisi√≥n y reduce alucinaciones.\n",
        "\n",
        "### ‚öôÔ∏è Flujo del proceso con LlamaIndex + RAG\n",
        "\n",
        "1. üó£Ô∏è **Consulta del Usuario**  \n",
        "   El usuario formula una pregunta o solicitud.\n",
        "\n",
        "2. üìÇ **Recuperaci√≥n de Contexto**  \n",
        "   El sistema consulta un √≠ndice previamente construido y selecciona los fragmentos de texto m√°s relevantes.\n",
        "\n",
        "3. üß© **Integraci√≥n de Datos**  \n",
        "   Se pueden combinar m√∫ltiples fuentes:  \n",
        "   - Datos estructurados (bases SQL, CSVs)  \n",
        "   - Datos no estructurados (PDFs, documentos)  \n",
        "   - Datos program√°ticos (APIs)\n",
        "\n",
        "4. ‚úçÔ∏è **Construcci√≥n del Prompt**  \n",
        "   Se genera un prompt enriquecido que contiene la pregunta original y los fragmentos recuperados como contexto adicional.\n",
        "\n",
        "5. ü§ñ **Generaci√≥n de la Respuesta**  \n",
        "   El LLM utiliza el contexto aportado para generar una respuesta precisa, sin depender √∫nicamente de su memoria interna.\n",
        "\n",
        "6. üì¨ **Entrega de la Respuesta**  \n",
        "   El modelo devuelve una respuesta contextualizada, combinando su conocimiento general con la informaci√≥n espec√≠fica recuperada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wxfcg6i5MP39",
      "metadata": {
        "id": "Wxfcg6i5MP39"
      },
      "source": [
        "# üõ†Ô∏è Pipeline RAG implementado con LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dEuTNG-wMYj3",
      "metadata": {
        "id": "dEuTNG-wMYj3"
      },
      "source": [
        "## üì¶ Instalaci√≥n de paquetes y configuraci√≥n de API Key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index llama-index-embeddings-jinaai llama-index-vector-stores-chroma llama-index-llms-huggingface-api --quiet\n",
        "%pip install requests beautifulsoup4 pdf2image PyPDF2 pytesseract --quiet\n",
        "!sudo apt install tesseract-ocr poppler-utils\n",
        "!sudo apt update\n",
        "!sudo apt install tesseract-ocr-spa"
      ],
      "metadata": {
        "id": "zCfyGRu0qqWc",
        "collapsed": true
      },
      "id": "zCfyGRu0qqWc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "vJOK_dW7M_Nt",
      "metadata": {
        "id": "vJOK_dW7M_Nt"
      },
      "source": [
        "## üì• Loading de Documentos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PgaJMrSPNcaY",
      "metadata": {
        "id": "PgaJMrSPNcaY"
      },
      "source": [
        "### üßæ ¬øQu√© se carga en LlamaIndex?\n",
        "Los documentos pueden provenir de **cualquier fuente de datos**, como: PDFs, Sitios web, Bases de datos o APIs\n",
        "\n",
        "### üîå Readers\n",
        "LlamaIndex utiliza componentes llamados **Readers** (tambi√©n conocidos como *Loaders* o *Connectors*) que:\n",
        "\n",
        "- ‚úÖ Permiten importar informaci√≥n desde m√∫ltiples formatos.\n",
        "- üìö Soportan datos: **Estructurados** (como tablas o bases SQL), **No estructurados** (como texto libre, PDFs, HTMLs) y **Program√°ticos** (como respuestas desde APIs).\n",
        "Transforman las fuentes originales en objetos `Document`, que contienen:\n",
        "   - El **contenido textual extra√≠do**.\n",
        "   - **Metadatos** como nombre de archivo, URL, t√≠tulo, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pVaUI9Kprlug",
      "metadata": {
        "collapsed": true,
        "id": "pVaUI9Kprlug"
      },
      "outputs": [],
      "source": [
        "import os, requests, tempfile, shutil, pytesseract\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "from pdf2image import convert_from_path\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import urljoin\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import re\n",
        "\n",
        "def download_pdf(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
        "    with open(tmp_file.name, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    return tmp_file.name\n",
        "\n",
        "def is_pdf_selectable(pdf_path):\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text and text.strip():\n",
        "                return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error leyendo PDF: {e}\")\n",
        "        return False\n",
        "\n",
        "def ocr_pdf_to_text(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    text = \"\"\n",
        "    for img in images:\n",
        "        text += pytesseract.image_to_string(img, lang=\"spa\") + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def process_pdf_from_url(url, file_name, output_dir=\".\"):\n",
        "    pdf_path = download_pdf(url)\n",
        "\n",
        "    base_name = Path(file_name).stem  # Ej: \"11071_0\"\n",
        "\n",
        "    output_txt_path = os.path.join(output_dir, f\"{base_name}.txt\")\n",
        "    output_pdf_path = os.path.join(output_dir, file_name)\n",
        "\n",
        "    if is_pdf_selectable(pdf_path):\n",
        "        print(\"‚úÖ PDF con texto seleccionable. Guardando sin modificar...\")\n",
        "        shutil.move(pdf_path, output_pdf_path)\n",
        "        print(f\"üìÑ PDF guardado en: {output_pdf_path}\")\n",
        "        return output_pdf_path\n",
        "    else:\n",
        "        print(\"üßæ PDF escaneado. Usando OCR y guardando como texto...\")\n",
        "        text = ocr_pdf_to_text(pdf_path)\n",
        "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        print(f\"üìù Texto OCR guardado en: {output_txt_path}\")\n",
        "        return output_txt_path\n",
        "\n",
        "def get_meta(file_path):\n",
        "    return {\n",
        "        \"carrera\": os.path.basename(os.path.dirname(file_path)),\n",
        "        \"asignatura\": os.path.splitext(os.path.basename(file_path))[0]\n",
        "    }\n",
        "\n",
        "def get_programas_dir(url, name):\n",
        "    PROGRAMAS_DIR = \"./programas/\" + name\n",
        "    return PROGRAMAS_DIR\n",
        "\n",
        "def get_programas_de_asignatura(url):\n",
        "    PAGINA_ASIGNATURA = url\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(PAGINA_ASIGNATURA)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al obtener la p√°gina: {e}\")\n",
        "        exit()\n",
        "\n",
        "    nombre_carrera = soup.find_all(\"h1\", class_=\"page-title\")[0].get_text(strip=True)\n",
        "    nombre_carrera = re.sub(r\"\\s*\\(.*?\\)\", \"\", nombre_carrera).strip()\n",
        "    print(f\"Nombre de la carrera: {nombre_carrera}\")\n",
        "\n",
        "    PROGRAMAS_DIR = get_programas_dir(url, nombre_carrera)\n",
        "\n",
        "    os.makedirs(PROGRAMAS_DIR, exist_ok=True)\n",
        "\n",
        "    links = soup.find_all(\"a\", href=True)\n",
        "    pdf_links = []\n",
        "\n",
        "    for link in links:\n",
        "        href = link[\"href\"]\n",
        "        if href.lower().endswith(\".pdf\") and \"/Programas/\" in href:\n",
        "            full_url = urljoin(PAGINA_ASIGNATURA, href)\n",
        "            texto = link.get_text(strip=True)  # tambi√©n pod√©s usar link.text.strip()\n",
        "            pdf_links.append((full_url, texto))\n",
        "\n",
        "    pdf_links = sorted(set(pdf_links))\n",
        "    print(f\"Enlaces PDF detectados: {len(pdf_links)}\")\n",
        "    print()\n",
        "\n",
        "    for url,file_name in pdf_links:\n",
        "        print(f\"Nombre de la asignatura: {file_name}\")\n",
        "        process_pdf_from_url(url, file_name, PROGRAMAS_DIR)\n",
        "        print()\n",
        "\n",
        "    reader_programas = SimpleDirectoryReader(PROGRAMAS_DIR, file_metadata = get_meta)\n",
        "    return reader_programas.load_data(), nombre_carrera\n",
        "\n",
        "def es_url_valida(url):\n",
        "    patron = r\"^https://www\\.certificaciones\\.unlu\\.edu\\.ar/\\?q=node/\\d+$\"\n",
        "    return re.match(patron, url) is not None\n",
        "\n",
        "\n",
        "documentos_dict = {}\n",
        "\n",
        "entrada = \"\"\n",
        "while entrada != \"SALIR\":\n",
        "    print(\"Las listas de programas de asignaturas tienen esta forma: 'https://www.certificaciones.unlu.edu.ar/?q=node/<X>'. X var√≠a dependiendo de la carrera\")\n",
        "    entrada = input(\"Ingrese la p√°gina web que lista las asignaturas: \")\n",
        "\n",
        "    if es_url_valida(entrada):\n",
        "        documentos_programa, nombre_carrera = get_programas_de_asignatura(entrada)\n",
        "        documents_dict[nombre_carrera] = documentos_programa\n",
        "    elif entrada.upper() != \"SALIR\":\n",
        "        print(\"URL inv√°lida. Intente nuevamente.\")\n",
        "print(\"Saliendo...\")\n",
        "\n",
        "# EJEMPLO DE URL A INGRESAR: https://www.certificaciones.unlu.edu.ar/?q=node/43"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Si se cuenta ya con los archivos textuales de los programas en el entorno de ejecuci√≥n, se puede ejecutar esta celda para no procesar nuevamente desde la p√°gina oficial de la asignatura"
      ],
      "metadata": {
        "id": "xyra-KkM1orS"
      },
      "id": "xyra-KkM1orS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S5gHE4TDKT7Q",
      "metadata": {
        "id": "S5gHE4TDKT7Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROGRAMAS_DIR = \"programas\"\n",
        "\n",
        "def get_meta(file_path):\n",
        "    return {\n",
        "        \"carrera\": os.path.basename(os.path.dirname(file_path)),\n",
        "        \"asignatura\": os.path.splitext(os.path.basename(file_path))[0]\n",
        "    }\n",
        "\n",
        "def cargar_programas_por_carrera():\n",
        "    documentos_por_carrera = {}\n",
        "    for nombre_carrera in os.listdir(PROGRAMAS_DIR):\n",
        "        ruta_carrera = os.path.join(PROGRAMAS_DIR, nombre_carrera)\n",
        "        if os.path.isdir(ruta_carrera):\n",
        "            reader_programas = SimpleDirectoryReader(ruta_carrera, file_metadata=get_meta)\n",
        "            documentos = reader_programas.load_data()\n",
        "            documentos_por_carrera[nombre_carrera] = documentos\n",
        "    return documentos_por_carrera\n",
        "\n",
        "documentos_dict = cargar_programas_por_carrera()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KFV0X8lSwt70",
      "metadata": {
        "id": "KFV0X8lSwt70"
      },
      "source": [
        "### Fragmentaci√≥n y Nodos\n",
        "Luego de obtener los objetos `Document`, estos son divididos en **Nodos**, que son las **unidades m√≠nimas de informaci√≥n**, fragmentos peque√±os y optimizados para b√∫squeda y recuperaci√≥n en el sistema RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hw-vyJdSrAhZ",
      "metadata": {
        "id": "Hw-vyJdSrAhZ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "def clean_text(text):\n",
        "    return text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
        "\n",
        "# Definir splitter\n",
        "splitter = SentenceSplitter(chunk_size=2048, chunk_overlap=500, include_metadata=True)\n",
        "\n",
        "# Obtener diccionario de nodos por carrera\n",
        "nodos_dict = {}\n",
        "for nombre_carrera, documentos in documentos_dict.items():\n",
        "    nodos_dict[nombre_carrera] = splitter.get_nodes_from_documents(documentos)\n",
        "    for nodo in nodos_dict[nombre_carrera]:\n",
        "        nodo.text = clean_text(nodo.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NqaZ0ZrVM51B",
      "metadata": {
        "id": "NqaZ0ZrVM51B"
      },
      "source": [
        "## üß≠ Indexing (Indexaci√≥n de Documentos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oGlBJVDTNkiI",
      "metadata": {
        "id": "oGlBJVDTNkiI"
      },
      "source": [
        "### üß† ¬øQu√© es el Indexado?\n",
        "La indexaci√≥n consiste en convertir los documentos cargados en **vectores de embeddings** (representaciones num√©ricas del significado de cada Nodo) y organizarlos en una estructura especial llamada **√≠ndice vectorial** *(Vector Store)*, que incluye tambi√©n los metadatos asociados.\n",
        "\n",
        "> üìå Cada Node del documento se transforma en un vector de embedding.\n",
        "\n",
        "### üîç ¬øPara qu√© sirve el √≠ndice?\n",
        "\n",
        "- Permite buscar por **significado**, no solo por coincidencia exacta.\n",
        "- Devuelve los **fragmentos m√°s relevantes** para una consulta.\n",
        "- Es la base para construir el **contexto que se le pasa al LLM** (en RAG).\n",
        "\n",
        "> üìå El √≠ndice es esencial para recuperar la informaci√≥n correcta en tiempo real y enriquecer las respuestas del modelo con datos espec√≠ficos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24aqk3v7DpFe",
      "metadata": {
        "id": "24aqk3v7DpFe"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.jinaai import JinaEmbedding\n",
        "\n",
        "# Definir embedding\n",
        "embed_model = JinaEmbedding(\n",
        "    model_name=\"jina-embeddings-v2-base-en\",\n",
        "    api_key=\"INGRESE_SU_API_KEY\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSeAsqQFCjO3",
      "metadata": {
        "id": "uSeAsqQFCjO3"
      },
      "source": [
        "## üíæ Storing (Almacenamiento de Embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5rf539JfNqHg",
      "metadata": {
        "id": "5rf539JfNqHg"
      },
      "source": [
        "### üß† ¬øD√≥nde se almacenan los embeddings?\n",
        "Una vez generados los embeddings de cada fragmento (Node), estos pueden guardarse en una **base de datos vectorial** como: **Chroma**, **FAISS**, **Weaviate**, **Pinecone**, entre otros\n",
        "\n",
        "Esto permite que el sistema sea **persistente** y **escalable**, sin tener que recalcular embeddings cada vez que se ejecuta.\n",
        "\n",
        "### üì¶ ¬øQu√© se almacena?\n",
        "\n",
        "En el proceso de almacenamiento, se guarda:\n",
        "- üìä **Embeddings**: el vector que representa el significado del nodo.\n",
        "- üè∑Ô∏è **Metadatos**: como nombre del archivo, p√°gina, secci√≥n, t√≠tulo, etc.\n",
        "- üîó **Relaci√≥n con el documento original**: para reconstruir f√°cilmente el contexto completo.\n",
        "\n",
        "### üß∞ ¬øPara qu√© sirve almacenar los embeddings?\n",
        "\n",
        "El almacenamiento en vector stores permite:\n",
        "\n",
        "- ‚úÖ **Evitar recomputar** embeddings en cada ejecuci√≥n.\n",
        "- üîÅ **Reutilizar el √≠ndice** en distintos procesos o notebooks.\n",
        "- üöÄ **Realizar consultas eficientes** sobre grandes vol√∫menes de informaci√≥n.\n",
        "- üß† **Mantener contexto** para el modelo sin procesar los documentos nuevamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q2-B9rgwB7Ry",
      "metadata": {
        "id": "q2-B9rgwB7Ry"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb, os, unicodedata, re\n",
        "\n",
        "def normalizar_nombre(nombre):\n",
        "    nombre = unicodedata.normalize('NFKD', nombre).encode('ascii', 'ignore').decode('ascii')\n",
        "    nombre = nombre.lower().replace(\" \", \"_\")\n",
        "    nombre = re.sub(r\"[^a-z0-9._-]\", \"\", nombre)\n",
        "    nombre = re.sub(r\"^[^a-z0-9]+\", \"\", nombre)\n",
        "    nombre = re.sub(r\"[^a-z0-9]+$\", \"\", nombre)\n",
        "    return nombre\n",
        "\n",
        "def create_index_from_documents(nodes, collection_name):\n",
        "    os.makedirs(\"./chroma_db\", exist_ok=True)\n",
        "    db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "    chroma_collection = db.get_or_create_collection(collection_name)\n",
        "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "    index = VectorStoreIndex(\n",
        "        nodes,\n",
        "        embed_model=embed_model,\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "    return index\n",
        "\n",
        "indexes = {}\n",
        "\n",
        "# Obtener diccionario de indices por carrera\n",
        "for nombre_carrera, nodes in nodos_dict.items():\n",
        "    nombre_index = normalizar_nombre(f\"programa_{nombre_carrera}\")\n",
        "    indexes[nombre_carrera] = create_index_from_documents(nodes, nombre_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GU-Rg_4eFbfU",
      "metadata": {
        "id": "GU-Rg_4eFbfU"
      },
      "source": [
        "## üîé Querying (Consulta y Generaci√≥n de Respuestas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZH0Lpg_nNunI",
      "metadata": {
        "id": "ZH0Lpg_nNunI"
      },
      "source": [
        "En esta etapa, el sistema utiliza m√∫ltiples componentes para **recuperar informaci√≥n relevante** desde el √≠ndice y generar una **respuesta contextualizada** con ayuda del LLM.\n",
        "\n",
        "### üß≠ 1. Retriever: Buscar Nodos Relevantes\n",
        "\n",
        "Un **Retriever** b√°sico se encarga de:\n",
        "\n",
        "1. üî¢ Convertir la consulta del usuario en un **embedding**.\n",
        "2. üìç Buscar los **Nodes m√°s cercanos** (m√°s relevantes) en el espacio vectorial del √≠ndice.\n",
        "3. üì¶ Devolver esos fragmentos como **contexto** para el modelo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "# CON HUGGING FACE VIA API\n",
        "HF_TOKEN = \"INGRESE_SU_API_KEY\"\n",
        "\n",
        "# Definimos el llm a utilizar como base para el RAG\n",
        "llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    token=HF_TOKEN,\n",
        "    provider=\"hf-inference\"\n",
        ")\n",
        "\n",
        "# Definimos las query_engine_tools, que contienen dentro los Retrievers\n",
        "tools = []\n",
        "for nombre_carrera, index in indexes.items():\n",
        "    query_engine = index.as_query_engine(\n",
        "        llm=llm,\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True\n",
        "    )\n",
        "    tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=query_engine,\n",
        "        description=f\"Informaci√≥n sobre los programas de las asignaturas de la carrera de {nombre_carrera} de la Universidad Nacional de Lujan. Incluye contenidos, equipo docente, condicion de regular y aprobado y correlatividad de cada materia/asignatura\"\n",
        "    )\n",
        "    tools.append(tool)"
      ],
      "metadata": {
        "id": "hZktChCGm33y"
      },
      "id": "hZktChCGm33y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "pminb01SBILE",
      "metadata": {
        "id": "pminb01SBILE"
      },
      "source": [
        "### üß† 2. Router: Elegir el √çndice Correcto\n",
        "\n",
        "Trabajamos con **m√∫ltiples fuentes de conocimiento** (En este caso: programas de asignaturas de distintas carreras), por lo que podemos usar un `Router`.\n",
        "\n",
        "üìå **¬øQu√© hace?**\n",
        "\n",
        "- Eval√∫a la consulta.\n",
        "- Selecciona autom√°ticamente el **Retriever** (y por lo tanto el √≠ndice) m√°s adecuado.\n",
        "- Redirige la consulta al √≠ndice m√°s relevante.\n",
        "\n",
        "> √ötil cuando ten√©s m√∫ltiples dominios o tipos de documentos en un mismo sistema.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors.llm_selectors import LLMSingleSelector\n",
        "\n",
        "# Creamos el router con un selector que elige solo uno de los Retriever especificados\n",
        "router = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(llm=llm),\n",
        "    query_engine_tools=tools,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "consulta = input(\"Ingrese su consulta: \")\n",
        "\n",
        "response = router.query(consulta)\n",
        "print(\"üí¨ Con RAG:\\n\", response)\n",
        "print()\n",
        "\n",
        "llm_noRAG = HuggingFaceInferenceAPI(\n",
        "    model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    token=HF_TOKEN,\n",
        "    provider=\"hf-inference\"\n",
        ")\n",
        "response = llm_noRAG.complete(consulta)\n",
        "print(\"üí¨ Sin RAG:\\n\", response)\n",
        "print()"
      ],
      "metadata": {
        "id": "yQTypGhT_ppG"
      },
      "id": "yQTypGhT_ppG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}